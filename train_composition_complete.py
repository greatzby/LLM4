# train_composition_fixed.py - ä¿®å¤ç‰ˆçš„å®Œæ•´è®­ç»ƒè„šæœ¬
# è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶ï¼ŒåŒ…å«äº†æ‰€æœ‰ä¿®å¤

import os
import time
import math
import pickle
import argparse
import numpy as np
import torch
import torch.nn.functional as F
import networkx as nx
from contextlib import nullcontext
import matplotlib.pyplot as plt
from datetime import datetime
from collections import defaultdict

from model import GPTConfig, GPT
from logger import get_logger

def parse_args():
    parser = argparse.ArgumentParser(description='Composition Ability Training')
    parser.add_argument('--experiment_name', type=str, default='composition', help='Experiment name')
    parser.add_argument('--total_nodes', type=int, default=90, help='Total nodes (3 stages)')
    parser.add_argument('--n_layer', type=int, default=1)
    parser.add_argument('--n_head', type=int, default=1)
    parser.add_argument('--n_embd', type=int, default=120)
    parser.add_argument('--max_iters', type=int, default=50000)
    parser.add_argument('--test_interval', type=int, default=1000)
    parser.add_argument('--device', type=str, default='cuda:0')
    parser.add_argument('--learning_rate', type=float, default=5e-4)
    parser.add_argument('--batch_size', type=int, default=1024)
    parser.add_argument('--train_paths_per_pair', type=int, default=10)
    parser.add_argument('--checkpoint_interval', type=int, default=2000)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--training_mode', type=str, default='standard', 
                      choices=['standard', 'mixed', 'curriculum'])
    parser.add_argument('--mixed_ratio', type=float, default=0.1)
    parser.add_argument('--temperature', type=float, default=0.8)
    parser.add_argument('--top_k', type=int, default=50)
    return parser.parse_args()

def set_seed(seed):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def encode(s, stoi):
    """ç¼–ç å­—ç¬¦ä¸²ä¸ºtokenåºåˆ—"""
    tokens = []
    for ch in s.split():
        if ch in stoi:
            tokens.append(stoi[ch])
    return tokens

def decode(l, itos):
    """è§£ç tokenåºåˆ—ä¸ºå­—ç¬¦ä¸²"""
    return " ".join([itos[i] for i in l if i in itos])

def classify_path_type(source, target, stages):
    """åˆ¤æ–­è·¯å¾„ç±»å‹"""
    S1, S2, S3 = stages
    if source in S1 and target in S2:
        return 'S1->S2'
    elif source in S2 and target in S3:
        return 'S2->S3'
    elif source in S1 and target in S3:
        return 'S1->S3'
    else:
        return 'other'

def check_path_validity(G, path_nodes):
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦æœ‰æ•ˆ"""
    if len(path_nodes) < 2:
        return False
    for i in range(len(path_nodes) - 1):
        if not G.has_edge(str(path_nodes[i]), str(path_nodes[i+1])):
            return False
    return True

def analyze_composition_error(pred_path, target_path, stages):
    """åˆ†æç»„åˆé”™è¯¯ç±»å‹"""
    S1, S2, S3 = stages
    
    if len(pred_path) < 2:
        return "too_short"
    
    pred_source, pred_target = pred_path[0], pred_path[-1]
    true_source, true_target = target_path[0], target_path[-1]
    
    if pred_source != true_source:
        return "wrong_source"
    if pred_target != true_target:
        return "wrong_target"
    
    # æ£€æŸ¥æ˜¯å¦ç»è¿‡äº†S2ï¼ˆå¯¹äºS1->S3è·¯å¾„ï¼‰
    if true_source in S1 and true_target in S3:
        has_s2 = any(node in S2 for node in pred_path[1:-1])
        if not has_s2:
            return "no_intermediate_stage"
    
    return "other"

@torch.no_grad()
def evaluate_tf_by_type(model, val_data, stages, stoi, itos, device, block_size, batch_size=64):
    """åœ¨éªŒè¯é›†ä¸Šåˆ†åˆ«è¯„ä¼°ä¸åŒè·¯å¾„ç±»å‹çš„Teacher Forcingå‡†ç¡®ç‡"""
    model.eval()
    
    S1, S2, S3 = stages
    data_size = block_size + 1
    
    # åˆå§‹åŒ–ç»Ÿè®¡
    results = {
        'S1->S2': {'correct': 0, 'total': 0},
        'S2->S3': {'correct': 0, 'total': 0},
        'S1->S3': {'correct': 0, 'total': 0},
        'overall': {'correct': 0, 'total': 0}
    }
    
    # éå†éªŒè¯æ•°æ®
    num_sequences = min(len(val_data) // data_size, 200)  # è¯„ä¼°å‰200ä¸ªåºåˆ—
    
    for seq_idx in range(num_sequences):
        start_idx = seq_idx * data_size
        seq = val_data[start_idx:start_idx + data_size]
        
        # æ‰¾åˆ°sourceå’Œtarget
        source_token = None
        target_token = None
        
        for i, token in enumerate(seq):
            if token > 1:  # ä¸æ˜¯PAD(0)æˆ–newline(1)
                if source_token is None:
                    source_token = token
                elif target_token is None:
                    target_token = token
                    break
        
        if source_token is None or target_token is None:
            continue
            
        # è½¬æ¢ä¸ºèŠ‚ç‚¹ID
        source = source_token - 2
        target = target_token - 2
        
        # åˆ¤æ–­è·¯å¾„ç±»å‹
        path_type = classify_path_type(source, target, stages)
        if path_type == 'other':
            continue
        
        # å‡†å¤‡è¾“å…¥
        x = torch.from_numpy(seq[:block_size].astype(np.int64)).unsqueeze(0).to(device)
        y = torch.from_numpy(seq[1:1+block_size].astype(np.int64)).unsqueeze(0).to(device)
        
        # è·å–é¢„æµ‹
        with torch.no_grad():
            logits, _ = model(x, y)
        
        preds = torch.argmax(logits, dim=-1)
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆåªåœ¨épaddingä½ç½®ï¼‰
        mask = y[0] != 0
        if mask.sum() > 0:
            correct = (preds[0][mask] == y[0][mask]).sum().item()
            total = mask.sum().item()
            
            results[path_type]['correct'] += correct
            results[path_type]['total'] += total
            results['overall']['correct'] += correct
            results['overall']['total'] += total
    
    # è®¡ç®—å‡†ç¡®ç‡
    for path_type in results:
        if results[path_type]['total'] > 0:
            results[path_type]['accuracy'] = results[path_type]['correct'] / results[path_type]['total']
        else:
            results[path_type]['accuracy'] = 0.0
    
    model.train()
    return results

@torch.no_grad()
def evaluate_ar_by_type(model, test_file, stages, stoi, itos, device, G, max_eval=50):
    """åœ¨æµ‹è¯•é›†ä¸Šä½¿ç”¨Autoregressiveç”Ÿæˆè¯„ä¼°ä¸åŒè·¯å¾„ç±»å‹"""
    model.eval()
    
    S1, S2, S3 = stages
    
    # è¯»å–æµ‹è¯•æ•°æ®
    with open(test_file, 'r') as f:
        test_lines = [line.strip() for line in f.readlines() if line.strip()]
    
    # åˆ†ç±»æµ‹è¯•æ•°æ®
    test_by_type = {
        'S1->S2': [],
        'S2->S3': [],
        'S1->S3': []
    }
    
    for line in test_lines:
        parts = line.split()
        if len(parts) >= 2:
            source, target = int(parts[0]), int(parts[1])
            path_type = classify_path_type(source, target, stages)
            if path_type in test_by_type:
                test_by_type[path_type].append(line)
    
    # ç»“æœç»Ÿè®¡
    results = {}
    
    for path_type, test_cases in test_by_type.items():
        results[path_type] = {
            'correct': 0,
            'total': min(len(test_cases), max_eval),
            'errors': defaultdict(int)
        }
        
        # è¯„ä¼°æ¯ä¸ªæµ‹è¯•æ¡ˆä¾‹
        for idx, test_line in enumerate(test_cases[:max_eval]):
            parts = test_line.split()
            source, target = parts[0], parts[1]
            true_path = [int(p) for p in parts]
            
            # å‡†å¤‡è¾“å…¥prompt
            prompt = f"{source} {target} {source}"
            prompt_ids = encode(prompt, stoi)
            
            if not prompt_ids:
                continue
                
            x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)
            
            # ç”Ÿæˆé¢„æµ‹
            try:
                with torch.no_grad():
                    y = model.generate(x, max_new_tokens=30, 
                                      temperature=args.temperature, 
                                      top_k=args.top_k)
                
                # è§£ç å¹¶è§£æ
                pred_str = decode(y[0].tolist(), itos)
                pred_parts = pred_str.split()
                
                # æå–é¢„æµ‹è·¯å¾„
                pred_path = []
                for p in pred_parts:
                    if p == '\n':
                        break
                    if p.isdigit():
                        pred_path.append(int(p))
                
                # æ£€æŸ¥æ­£ç¡®æ€§
                is_correct = False
                if len(pred_path) >= 2:
                    if pred_path[0] == int(source) and pred_path[-1] == int(target):
                        # æ£€æŸ¥è·¯å¾„æœ‰æ•ˆæ€§
                        if check_path_validity(G, pred_path):
                            is_correct = True
                            results[path_type]['correct'] += 1
                
                # å¦‚æœé”™è¯¯ï¼Œåˆ†æé”™è¯¯ç±»å‹
                if not is_correct:
                    error_type = analyze_composition_error(pred_path, true_path, stages)
                    results[path_type]['errors'][error_type] += 1
            except Exception as e:
                results[path_type]['errors']['generation_error'] += 1
        
        # è®¡ç®—å‡†ç¡®ç‡
        if results[path_type]['total'] > 0:
            results[path_type]['accuracy'] = results[path_type]['correct'] / results[path_type]['total']
        else:
            results[path_type]['accuracy'] = 0.0
    
    model.train()
    return results

def main():
    global args  # è®©evaluate_ar_by_typeèƒ½è®¿é—®args
    args = parse_args()
    set_seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = f'out/composition_{args.training_mode}_{timestamp}'
    os.makedirs(out_dir, exist_ok=True)
    
    # è®¾ç½®logger
    logger = get_logger(os.path.join(out_dir, "train.log"))
    
    print("="*60)
    print(f"Composition Ability Training")
    print(f"Mode: {args.training_mode}")
    print(f"Model: {args.n_layer}L-{args.n_head}H-{args.n_embd}D")
    print("="*60)
    
    # è®°å½•é…ç½®
    logger.info("="*60)
    logger.info(f"Configuration: {vars(args)}")
    logger.info("="*60)
    
    # åŠ è½½æ•°æ®å’Œå…ƒä¿¡æ¯
    data_dir = os.path.join('data', 'simple_graph', f'{args.experiment_name}_{args.total_nodes}')
    
    # åŠ è½½é˜¶æ®µä¿¡æ¯
    with open(os.path.join(data_dir, 'stage_info.pkl'), 'rb') as f:
        stage_info = pickle.load(f)
    stages = stage_info['stages']
    S1, S2, S3 = stages
    
    print(f"Stage info: S1={S1[:3]}..., S2={S2[:3]}..., S3={S3[:3]}...")
    
    # åŠ è½½å…ƒä¿¡æ¯
    with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:
        meta = pickle.load(f)
    
    stoi, itos = meta['stoi'], meta['itos']
    block_size = meta['block_size']
    vocab_size = meta['vocab_size']
    
    # åŠ è½½å›¾
    G = nx.read_graphml(os.path.join(data_dir, 'composition_graph.graphml'))
    print(f"Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
    
    # åŠ è½½æ•°æ®
    train_data = np.memmap(os.path.join(data_dir, f'train_{args.train_paths_per_pair}.bin'), 
                          dtype=np.uint16, mode='r')
    val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')
    test_file = os.path.join(data_dir, 'test.txt')
    
    print(f"Data loaded: train={len(train_data)//block_size} sequences, val={len(val_data)//block_size} sequences")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model_args = dict(
        n_layer=args.n_layer,
        n_head=args.n_head,
        n_embd=args.n_embd,
        block_size=block_size,
        bias=False,
        vocab_size=vocab_size,
        dropout=0.0
    )
    
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf).to(args.device)
    
    print(f"Model initialized: {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters")
    
    # ä¼˜åŒ–å™¨
    optimizer = model.configure_optimizers(
        weight_decay=1e-1,
        learning_rate=args.learning_rate,
        betas=(0.9, 0.95),
        device_type='cuda' if 'cuda' in args.device else 'cpu'
    )
    
    # æ•°æ®åŠ è½½å‡½æ•°
    def get_batch(split):
        data = train_data if split == 'train' else val_data
        data_size = block_size + 1
        
        # ç¡®ä¿å¯¹é½åˆ°åºåˆ—è¾¹ç•Œ
        num_sequences = len(data) // data_size
        seq_indices = torch.randint(0, num_sequences, (args.batch_size,))
        ix = seq_indices * data_size
        
        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
        
        return x.to(args.device), y.to(args.device)
    
    # è®­ç»ƒå†å²
    history = {
        'iter': [],
        'train_loss': [],
        'val_loss': [],
        # Teacher Forcing å‡†ç¡®ç‡ï¼ˆéªŒè¯é›†ï¼‰
        'tf_overall': [],
        'tf_s1_s2': [],
        'tf_s2_s3': [],
        'tf_s1_s3': [],
        # Autoregressive å‡†ç¡®ç‡ï¼ˆæµ‹è¯•é›†ï¼‰
        'ar_s1_s2': [],
        'ar_s2_s3': [],
        'ar_s1_s3': [],
        # é”™è¯¯åˆ†æ
        's1_s3_errors': []
    }
    
    # è®­ç»ƒå¾ªç¯
    print("\nStarting training...")
    print(f"Evaluation: TF on validation set, AR on test set")
    running_loss = 0
    loss_count = 0
    
    for iter_num in range(args.max_iters + 1):
        # å­¦ä¹ ç‡è°ƒåº¦
        lr = args.learning_rate
        if iter_num < 2000:
            lr = args.learning_rate * iter_num / 2000
        elif iter_num > args.max_iters * 0.9:
            lr = args.learning_rate * 0.1
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        
        # å®šæœŸè¯„ä¼°
        if iter_num % args.test_interval == 0:
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = running_loss / loss_count if loss_count > 0 else 0
            
            # éªŒè¯é›†æŸå¤±
            model.eval()
            val_losses = []
            for _ in range(10):
                X_val, Y_val = get_batch('val')
                with torch.no_grad():
                    _, loss = model(X_val, Y_val)
                val_losses.append(loss.item())
            val_loss = np.mean(val_losses)
            
            # Teacher Forcing è¯„ä¼°ï¼ˆéªŒè¯é›†ï¼‰
            tf_results = evaluate_tf_by_type(model, val_data, stages, stoi, itos, args.device, block_size)
            
            # Autoregressive è¯„ä¼°ï¼ˆæµ‹è¯•é›†ï¼‰
            ar_results = evaluate_ar_by_type(model, test_file, stages, stoi, itos, args.device, G)
            
            # è®°å½•å†å²
            history['iter'].append(iter_num)
            history['train_loss'].append(avg_train_loss)
            history['val_loss'].append(val_loss)
            
            # TFå‡†ç¡®ç‡
            history['tf_overall'].append(tf_results['overall']['accuracy'])
            history['tf_s1_s2'].append(tf_results['S1->S2']['accuracy'])
            history['tf_s2_s3'].append(tf_results['S2->S3']['accuracy'])
            history['tf_s1_s3'].append(tf_results['S1->S3']['accuracy'])
            
            # ARå‡†ç¡®ç‡
            history['ar_s1_s2'].append(ar_results['S1->S2']['accuracy'])
            history['ar_s2_s3'].append(ar_results['S2->S3']['accuracy'])
            history['ar_s1_s3'].append(ar_results['S1->S3']['accuracy'])
            history['s1_s3_errors'].append(ar_results['S1->S3']['errors'])
            
            # æ‰“å°ç»“æœ
            print(f"\n{'='*60}")
            print(f"Iteration {iter_num}:")
            print(f"  Loss: train={avg_train_loss:.4f}, val={val_loss:.4f}")
            
            print(f"\n  Teacher Forcing (Validation Set):")
            print(f"    Overall: {tf_results['overall']['accuracy']:.2%}")
            print(f"    S1->S2: {tf_results['S1->S2']['accuracy']:.2%} ({tf_results['S1->S2']['total']} tokens)")
            print(f"    S2->S3: {tf_results['S2->S3']['accuracy']:.2%} ({tf_results['S2->S3']['total']} tokens)")
            print(f"    S1->S3: {tf_results['S1->S3']['accuracy']:.2%} ({tf_results['S1->S3']['total']} tokens)")
            
            print(f"\n  Autoregressive (Test Set):")
            print(f"    S1->S2: {ar_results['S1->S2']['accuracy']:.2%} ({ar_results['S1->S2']['correct']}/{ar_results['S1->S2']['total']})")
            print(f"    S2->S3: {ar_results['S2->S3']['accuracy']:.2%} ({ar_results['S2->S3']['correct']}/{ar_results['S2->S3']['total']})")
            print(f"    S1->S3: {ar_results['S1->S3']['accuracy']:.2%} ({ar_results['S1->S3']['correct']}/{ar_results['S1->S3']['total']})")
            
            # S1->S3é”™è¯¯åˆ†æ
            if ar_results['S1->S3']['errors']:
                print(f"\n  S1->S3 Error Analysis:")
                for error_type, count in ar_results['S1->S3']['errors'].items():
                    print(f"    {error_type}: {count}")
            
            # ç»„åˆèƒ½åŠ›è¯„ä¼°
            if ar_results['S1->S2']['accuracy'] > 0.8 and ar_results['S2->S3']['accuracy'] > 0.8:
                if ar_results['S1->S3']['accuracy'] < 0.2:
                    print("\n  âš ï¸  Poor composition ability despite good basic performance!")
            
            # è®°å½•æ—¥å¿—
            logger.info(f"Iter {iter_num}: loss={avg_train_loss:.4f}, "
                       f"TF: S1->S3={tf_results['S1->S3']['accuracy']:.2%}, "
                       f"AR: S1->S3={ar_results['S1->S3']['accuracy']:.2%}")
            
            running_loss = 0
            loss_count = 0
            model.train()
        
        # ä¿å­˜checkpoint
        if iter_num % args.checkpoint_interval == 0 and iter_num > 0:
            checkpoint = {
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'model_args': model_args,
                'iter_num': iter_num,
                'history': history,
                'config': vars(args)
            }
            checkpoint_path = os.path.join(out_dir, f'ckpt_{iter_num}.pt')
            torch.save(checkpoint, checkpoint_path)
            print(f"  Checkpoint saved to {checkpoint_path}")
        
        if iter_num == 0:
            continue
        
        # è®­ç»ƒæ­¥
        X, Y = get_batch('train')
        
        logits, loss = model(X, Y)
        
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        running_loss += loss.item()
        loss_count += 1
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(os.path.join(out_dir, 'history.pkl'), 'wb') as f:
        pickle.dump(history, f)
    
    # ç»˜åˆ¶å›¾è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
    plt.figure(figsize=(15, 10))
    
    # 1. è®­ç»ƒæŸå¤±
    plt.subplot(2, 3, 1)
    plt.plot(history['iter'], history['train_loss'], 'b-', label='Train')
    plt.plot(history['iter'], history['val_loss'], 'r-', label='Val')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()
    plt.grid(True)
    
    # 2. TFå‡†ç¡®ç‡
    plt.subplot(2, 3, 2)
    plt.plot(history['iter'], history['tf_s1_s2'], 'b-', label='S1->S2', marker='o')
    plt.plot(history['iter'], history['tf_s2_s3'], 'g-', label='S2->S3', marker='s')
    plt.plot(history['iter'], history['tf_s1_s3'], 'r-', label='S1->S3', marker='^')
    plt.xlabel('Iteration')
    plt.ylabel('Accuracy')
    plt.title('Teacher Forcing by Path Type')
    plt.legend()
    plt.grid(True)
    
    # 3. ARå‡†ç¡®ç‡
    plt.subplot(2, 3, 3)
    plt.plot(history['iter'], history['ar_s1_s2'], 'b-', label='S1->S2', marker='o')
    plt.plot(history['iter'], history['ar_s2_s3'], 'g-', label='S2->S3', marker='s')
    plt.plot(history['iter'], history['ar_s1_s3'], 'r-', label='S1->S3', marker='^', linewidth=2)
    plt.xlabel('Iteration')
    plt.ylabel('Accuracy')
    plt.title('Autoregressive by Path Type')
    plt.legend()
    plt.grid(True)
    
    # 4. S1->S3å¯¹æ¯”
    plt.subplot(2, 3, 4)
    plt.plot(history['iter'], history['tf_s1_s3'], 'b-', label='TF', marker='o')
    plt.plot(history['iter'], history['ar_s1_s3'], 'r-', label='AR', marker='s')
    plt.xlabel('Iteration')
    plt.ylabel('Accuracy')
    plt.title('S1->S3: TF vs AR')
    plt.legend()
    plt.grid(True)
    
    # 5. ç»„åˆèƒ½åŠ›å·®è·
    plt.subplot(2, 3, 5)
    if len(history['ar_s1_s2']) > 0:
        basic_avg = [(h1 + h2) / 2 for h1, h2 in zip(history['ar_s1_s2'], history['ar_s2_s3'])]
        composition_gap = [b - c for b, c in zip(basic_avg, history['ar_s1_s3'])]
        plt.plot(history['iter'], composition_gap, 'purple', linewidth=2)
        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)
        plt.xlabel('Iteration')
        plt.ylabel('Gap')
        plt.title('Composition Gap (Basic Avg - S1->S3)')
        plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, 'composition_results.png'), dpi=150)
    plt.close()
    
    # æ‰“å°æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*60}")
    print("Training Complete!")
    print(f"\nFinal Results:")
    print(f"\nTeacher Forcing (Validation Set):")
    print(f"  S1->S2: {history['tf_s1_s2'][-1]:.2%}")
    print(f"  S2->S3: {history['tf_s2_s3'][-1]:.2%}")
    print(f"  S1->S3: {history['tf_s1_s3'][-1]:.2%} â† Composition (TF)")
    
    print(f"\nAutoregressive (Test Set):")
    print(f"  S1->S2: {history['ar_s1_s2'][-1]:.2%}")
    print(f"  S2->S3: {history['ar_s2_s3'][-1]:.2%}")
    print(f"  S1->S3: {history['ar_s1_s3'][-1]:.2%} â† Composition (AR)")
    
    # ç»„åˆèƒ½åŠ›è¯„ä¼°
    if len(history['ar_s1_s2']) > 0:
        basic_performance = (history['ar_s1_s2'][-1] + history['ar_s2_s3'][-1]) / 2
        composition_performance = history['ar_s1_s3'][-1]
        
        print(f"\nComposition Analysis:")
        print(f"  Basic Path Average: {basic_performance:.2%}")
        print(f"  Composition Performance: {composition_performance:.2%}")
        print(f"  Composition Gap: {basic_performance - composition_performance:.2%}")
        
        if composition_performance < 0.1 and basic_performance > 0.8:
            print("\nâš ï¸  Model shows severe lack of compositional generalization!")
        elif composition_performance > 0.5:
            print("\nâœ… Model demonstrates reasonable compositional ability!")
        else:
            print("\nğŸ”¶ Model shows limited compositional ability.")
    
    print(f"\nResults saved to: {out_dir}")

if __name__ == "__main__":
    main()